---
title: "Meta-analysis in R"
format: html
editor: visual
---

```{r}
#| echo: false
#| message: false
#| warning: false
#check and load libraries
library(tidyverse, quietly = TRUE)
library(metafor, quietly = TRUE)
library(orchaRd, quietly = TRUE)
```

## Meta-analysis

Meta-analysis is the quantitative summary of effect sizes extracted from "published" sources. A systematic review doesn't have to have a meta-analysis (but a meta-analysis needs a systematic review).

## What is an effect size?

An effect size is the size of the effect!

The easiest way to think about them is to think of a classic experiment where we compare the effect of an intervention by using a randomised controlled trial.

We can simulate some data from a ten experiments (studies) to show the difference between an intervention and a control group. Let's imagine we are studying the effect of a restoration intervention on the species richness of beetles.

```{r}
#| echo: true
#| message: false
data <- data.frame(
  study_id = c(1:10,10),
  # Mean species richness in restored and control sites
  mean_restored = c(12, 15, 14, 20, 8, 13, 16, 19, 10, 18, 17),
  mean_control = c(9, 12, 11, 15, 10, 11, 14, 17, 9, 14, 14),
  # Standard deviations for restored and control groups
  sd_restored = c(2.1, 3.0, 2.5, 3.5, 2.0, 2.8, 3.2, 2.9, 2.3, 3.1, 3),
  sd_control = c(2.0, 2.9, 2.6, 3.2, 1.9, 2.5, 2.9, 2.7, 2.1, 2.8, 2.7),
  # Sample sizes for restored and control groups
  n_restored = c(30, 45, 40, 50, 25, 38, 60, 48, 35, 42, 42),
  n_control = c(30, 45, 40, 50, 25, 38, 60, 48, 35, 42, 42)
)

data |> head()


```

### Step 1: Calculating the Standardized Mean Difference (SMD)

The **standardized mean difference (SMD)** between restored and control sites is calculated as follows:

$$\text{SMD} = \frac{\text{mean}_{\text{restored}} - \text{mean}_{\text{control}}}{\text{pooled standard deviation}}$$ Where the **pooled standard deviation** is calculated as:

$$2SD_{\text{pooled}} = \sqrt{\frac{(n_{\text{restored}} - 1) \cdot SD_{\text{restored}}^2 + (n_{\text{control}} - 1) \cdot SD_{\text{control}}^2}{n_{\text{restored}} + n_{\text{control}} - 2}}$$

Let's calculate this in R:

```{r}
#| echo: true
#| message: false
# Calculate pooled standard deviation and SMD for each study
data$sd_pooled <- with(data, sqrt(((n_restored - 1) * sd_restored^2 + (n_control - 1) * sd_control^2) / (n_restored + n_control - 2)))
data$smd <- with(data, (mean_restored - mean_control) / sd_pooled)

# Calculate variance of the SMD
data$smd_variance <- with(data, ((n_restored + n_control) / (n_restored * n_control)) + (smd^2 / (2 * (n_restored + n_control))))

# View the calculated effect sizes and variances
print(data[c("study_id", "smd", "smd_variance")])

```

In ecology we often have small sample sizes, so we use Hedges g to "correct" for small samples. In the R package metafor we can calculate this:

```{r}
#| echo: true
#| message: false
hedges_g_data <- metafor::escalc(measure = "SMD",
                        m1i = mean_restored, sd1i = sd_restored, n1i = n_restored,
                        m2i = mean_control, sd2i = sd_control, n2i = n_control,
                        data = data)

# View the results
hedges_g_data |> 
  select(study_id, yi, vi) |> 
  head()
```

## Running a meta-analysis

### fixed effects vs random effects

In meta-analysis, fixed-effects models assume that all studies estimate the same underlying effect size, with any differences among study results being solely due to sampling error. This model is appropriate when we believe that the effect size is constant across all studies, meaning each study measures exactly the same phenomenon.

In contrast, random-effects models assume that the true effect size varies from study to study due to real differences in study characteristics, such as differences in species, locations, or experimental conditions. Random-effects models treat these variations as random and allow for both within-study sampling error and between-study variability. This is often more realistic in ecology, where conditions across studies are rarely identical.

### Accounting for non-independence

Where multiple effects come from a single study (or even from related species!) we need to account for non-independence. We can do this in the meta-analysis model.

```{r}
#| echo: true
#| message: false
ran_eff_model<-
  rma.mv(yi, 
         vi, 
         random = ~ 1 | study_id, 
         data = hedges_g_data)
```

```{r}
#| echo: true
#| message: false
#| warning: false
summary(ran_eff_model)
```

## Forest plot

```{r}
#| echo: true
#| message: false
#| warning: false
forest(ran_eff_model)
```

## Orchard plots

```{r}
#| echo: true
#| message: false
#| warning: false
orchard_plot(ran_eff_model, mod = "1", xlab = "Hedges' g", angle = 45, group="study_id")
```

## Pitfalls with Hedges g

It is very easy to make a mistake in extracting data for calulating the Hedges g. As a "rule of thumb" a Hedges g of greater than 5 should be double/tripple checked.

[Here is a link to my Hedges g checker](https://drmatt.shinyapps.io/Hedges_g_checker/)

## Cumulative meta-analysis

A **cumulative meta-analysis** examines how the overall effect size estimate changes as each study is added sequentially, usually sorted by publication date. This approach can reveal trends over time, showing whether the effect size has stabilised as more studies are included.

Let’s go through an example of performing a cumulative meta-analysis with the `metafor` package in R.

### Step 1: Create the Dataset

First, we’ll create a dataset with a `year` variable to sort studies in chronological order for the cumulative analysis.

```{r}
#| echo: true
#| message: false
#| warning: false
data <- data.frame(
  study_id = 1:10,
  year = c(2000, 2002, 2004, 2005, 2006, 2008, 2010, 2012, 2015, 2018),  # Publication year
  yi = c(0.8, 0.7, 0.5, 0.3, 0.1, -0.1, -0.3, -0.5, -0.7, -0.9),  # Effect sizes (Hedges' g)
  vi = c(0.02, 0.03, 0.025, 0.04, 0.018, 0.027, 0.035, 0.033, 0.02, 0.03)  # Variances
)

# Sort data by year for cumulative meta-analysis
data <- data[order(data$year), ]

```

### Step 2: Perform Cumulative Meta-Analysis

Now, we can use the `cumul()` function from `metafor` to perform a cumulative meta-analysis. This function recalculates the meta-analysis result each time a new study is added.

```{r}
#| echo: true
#| message: false
#| warning: false
# Fit an initial random-effects model 
random_effect_model <- rma(yi = yi, vi = vi, data = data, method = "REML")  # Perform cumulative meta-analysis 
cumulative_results <- cumul(random_effect_model, order = data$year)  # View cumulative meta-analysis 
cumulative_results

```

### Step 3: Plot the Cumulative Meta-Analysis

You can also visualize the cumulative effect size estimates over time using `forest()`.

```{r}
#| echo: true
#| message: false
#| warning: false
# Plot cumulative meta-analysis results 
forest(cumulative_results, xlab = "Cumulative Hedges' g", refline = 0, cex = 0.8)
```

### Interpretation

As you add each study (from the earliest to the most recent), you’ll see how the cumulative estimate of Hedges' g changes. In ecology, this can highlight if early results were consistent with later studies or if new findings led to significant shifts in the overall effect estimate. This approach provides insight into whether further studies are likely to meaningfully alter the cumulative effect size or if it has stabilised.
